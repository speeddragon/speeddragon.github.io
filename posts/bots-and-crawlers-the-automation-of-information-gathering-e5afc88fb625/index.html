<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Bots and Crawlers — The automation of information gathering | SpeedDragon Blog</title><meta name=keywords content><meta name=description content="Nowadays, with the fast pacing appearance of services on the Internet, we are faced constantly with new challenges. More commonly we are…"><meta name=author content><link rel=canonical href=https://speeddragon.github.io/posts/bots-and-crawlers-the-automation-of-information-gathering-e5afc88fb625/><link crossorigin=anonymous href=/assets/css/stylesheet.5861a6fea64e1e0784f794fd89c8f6fe71ac0528e3d0ef74da2ab896e25be14a.css integrity="sha256-WGGm/qZOHgeE95T9icj2/nGsBSjj0O902iq4luJb4Uo=" rel="preload stylesheet" as=style><link rel=icon href=https://speeddragon.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://speeddragon.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://speeddragon.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://speeddragon.github.io/apple-touch-icon.png><link rel=mask-icon href=https://speeddragon.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://speeddragon.github.io/posts/bots-and-crawlers-the-automation-of-information-gathering-e5afc88fb625/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script async src="https://www.googletagmanager.com/gtag/js?id=G-7KQHSZCH69"></script><script>var dnt,doNotTrack=!1;if(!1&&(dnt=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,doNotTrack=dnt=="1"||dnt=="yes"),!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-7KQHSZCH69")}</script><meta property="og:url" content="https://speeddragon.github.io/posts/bots-and-crawlers-the-automation-of-information-gathering-e5afc88fb625/"><meta property="og:site_name" content="SpeedDragon Blog"><meta property="og:title" content="Bots and Crawlers — The automation of information gathering"><meta property="og:description" content="Nowadays, with the fast pacing appearance of services on the Internet, we are faced constantly with new challenges. More commonly we are…"><meta property="og:locale" content="en-us"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2019-12-11T12:59:13+00:00"><meta property="article:modified_time" content="2019-12-11T12:59:13+00:00"><meta property="og:image" content="https://speeddragon.github.io/images/1__lwXZvQJ7eF__pI8tI1ernCw.jpeg"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://speeddragon.github.io/images/1__lwXZvQJ7eF__pI8tI1ernCw.jpeg"><meta name=twitter:title content="Bots and Crawlers — The automation of information gathering"><meta name=twitter:description content="Nowadays, with the fast pacing appearance of services on the Internet, we are faced constantly with new challenges. More commonly we are…"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://speeddragon.github.io/posts/"},{"@type":"ListItem","position":2,"name":"Bots and Crawlers — The automation of information gathering","item":"https://speeddragon.github.io/posts/bots-and-crawlers-the-automation-of-information-gathering-e5afc88fb625/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Bots and Crawlers — The automation of information gathering","name":"Bots and Crawlers — The automation of information gathering","description":"Nowadays, with the fast pacing appearance of services on the Internet, we are faced constantly with new challenges. More commonly we are…","keywords":[],"articleBody":"Nowadays, with the fast pacing appearance of services on the Internet, we are faced constantly with new challenges. More commonly we are seeing new types of attacks to companies that hold customer information. As today, this is an asset that keeps increasing in value.\nIn this article I will introduce you to my view and experience of gathering information since my early days of programming up until now, how it evolved, some challenges I’ve been faced with, and my view of the future.\nEarly Days Since my first steps on the Internet, I remember bots being present in the famous IRC clients and chatrooms. I was fascinated by how we could automate actions, pre-program some settings to help us complete some specific action and set up rules and parameters to help us reduce the time needed to perform what would be a manual action. These were my early days, the days I first saw a line of code and was something completely strange for me.\nWhat is a Bot ? An Internet bot, also known as web robot, WWW robot or simply bot, is a software application that runs automated tasks (scripts) over the Internet. Typically, bots perform tasks that are both simple and structurally repetitive, at a much higher rate than would be possible for a human alone.\nI remember questioning myself how the dialogs were created. The first time I “created” a dialog I think it was in MS Paint, using a print screen of a window (normally Internet Explorer, because it was the “portal” for accessing the Internet) and started adding buttons and changing text and input fields.\nI didn’t create many dialogs in mIRC, but I was fascinated how to automate a command, that could automatically reply to people messages. Creating a bot to reply to someone if a certain keyword was present was fun and then I started creating more complex actions in order to automate some repetitive tasks.\nWhen I stopped using mIRC, I remember that people created a lot of complex text games (maybe the predecessor of Omerta and other first internet web based games).\nWhat is a Crawler ? A crawler is a program that visits Web sites and reads their pages and other information in order to create entries for a search engine index. The major search engines on the Web all have such a program, which is also known as a “spider” or a “bot.”\nNormally crawlers are used in search engine to fetch information to be displayed on future searches. It used to be only GET requests that were indexed, but since 2011 that Google Bot also perform POST request to retrieve information, reaching further into the amount of information that can be gathered.\nThe growth of information access via Internet I start realising that the Internet was getting bigger and bigger, and a lot of systems were starting to become available to access them through a browser. Then I started to see bots being created to scrape text out of the server’s responses (well, that was how Google started) and people started building applications that gathered information from different systems, ultimately providing a service or an API out of it.\nAfter a while, companies realised that some actions needed to be taken, in order to check if the request was made by a human or a robot. In the early days, performing OCR on a CAPTCHA or understand what was inside of an image was pretty hard to do with a common computer, but soon that become an interest for the researchers and a problem easily overcome with the additional computational power available in the following years.\nGoogle ReCaptcha is probably the most widely used system today to prevent robots to access areas of websites that are meant to be accessed by humans only. There are other solutions, some of them are easy to bypass, and some that are very difficult for both humans and robots to solve it.\nStages of developing an information gathering service You can split the development of a bot/crawler into several stages. The first one starts with the planning, then you have the process of extracting the information and storage. Then you start to know how the website behaves and learn how to run it reliable 24/7.\nAdditional steps can be taken in order to help you visualise or learn more about the information that you are gathering.\nStage 1: Information recognition The first stage is to understand what information is available to you. Then you will learn how the website behaves, and pinpoint every detail that can be useful into the process of developing your bot.\nCheck every visual information that you can extract (text, images, videos, …). Check for hidden information that you can extract. Not all information is visible to the naked eye and sometimes you need to lurk in the source code to catch some extra information hidden in form fields or with CSS. Check your browser network tab for calls to an API, that may contain extra information that isn’t displayed in the website. In the olds days client side rendering wasn’t a major standard as it is today, so normally what you saw was what you got. Nowadays REST APIs are everywhere and a lot of information is returned in these calls that aren’t displayed visually to the end user. Check if you can differ an empty/invalid response from a not working service response. In some services the response of not having any information to show is the same as the service being down. The best way to recognise this is to log every request made and search for patterns in the data stored. This will also reveal possible service schedule maintenance patterns that might occur during the collection process. Check for possible visible or invisible automation detection system (like ReCaptcha, CloudFlare, etc). This will help assess how viable it will be to implement a crawler/bot on that page. At the end of this step, you should have all the necessary information to model your data into tables and columns in your database. Depending how much data you plan to extract, it might be good to have some experience in database optimisation, or to start reading about it.\nStage 2: Mimic browser behaviour This could be a painful process depending how many requests exists and how many static/dynamic fields you need to send back for verification / validation purposes. At this stage you should select your favourite programming language and start using an HTTP client to try to mimic the browser behaviour.\nTo help you in this hard task, you have two great tools at your disposal: the browser inspect (Chrome, Firefox and Safari) and Burp Suite Community Edition.\nYou should start by removing the headers parameters until you have the necessary minimum data to send in order to obtain the right response, but sometimes you can’t change anything. It might be picky and that can drive you insane until you have right information to be sent, especially if the service changes its behaviour.\nAccept-Language: pt-PT,pt;q=0.8,en;q=0.5,en-US;q=0.3\nAccept: text/html,application/xhtml+xml,application/xml; q=0.9,*/*; q=0.8\nBrowser Inspect When you start analysing the network requests, remember that you should open the network tab before the page is loaded, or if it was already loaded, make sure you refresh it so you can catch all network requests made inside that tab.\nInside the browser inspection tool, you will see which data is sent and received between your browser and the server, which websites it connects to, cookies, parameters sent, etc. Normally you want to find the request that have the information you want to extract (your goal), and them backtrack all the necessary steps from there, like for example search for early requests being made that must be replicated in your crawler.\nIn the majority of the cases you just need to know which request type is it (GET or POST, the most common ones) and which endpoint you are connecting to (URL), as well which parameters you need to input to extract the result returned by the server. Typically, this could be a form-data format like key=value\u0026key2=value2 or a JSON format string input {\"key\": \"value\", \"key2\": \"value2\"}.\nIf this doesn’t work, the next step is to investigate the cookies, and check if you need to do a previous request in order to generate the necessary cookies for the server to send the reply you are expecting. Sometimes this could be a long and painfully processes, so another way to do this is to export the request that return the information you want into a cURL command (on Chrome, right click in the request you want and select Copy \u003e Copy as cURL), and check in your command line if that works. Then you start to remove what you think might be unnecessary data until the response isn’t the same. That’s how you find out what fields are required or not.\nBurp Suite Community Edition Burp is probably the best tool to help you understand what information is being sent from your browser into the server, using a proxy.\nThis is a powerful tool used by hackers, security researches, and other IT personal, in order to easily access all the requests being sent by your browser, as well as utilise the handful of features that allows them to tamper with requests and resend them, in order to better understand the server’s behaviour.\nStage 3: Information extraction automation After you have identified the data to extract and have it in your side (replicating the browser behaviour), the next step is to store it into your database.\nDon’t forget to sanitise all information retrieved from the website / API. You shouldn’t trust the information that you extracted, and you should handle it as “user input”. If you display it in a HTML page, convert all input into html entities. Don’t ever concatenate extracted data into a SQL query, use prepare statements. Try to follow security standards like a normal web application. Use a DOM parser with query selectors to extract the information if the response is in HTML format. If the response is in JSON format use the decoder available for your language. For example, in PHP you have json_decode($input) . There might be some cases, like Facebook where parsing HTML as an HTTP request might be difficult, because everything is rendered on the client side with complex/obfuscated JavaScript code. Since the Cambridge Analytic scandal, Facebook has been locking access to its GraphQL API data, making it harder and harder to get “clean” data from it.\nThe solution for this issues is to use Puppeteer, a API to the Google Chrome browser that allows you to control it using NodeJS. Its behaviour is similar as doing an HTTP request, but with much more functionality because of the JavaScript being executed in the client side. Absolutely worthwhile looking into it.\nThe following code is a NodeJS example of how to extract the biggest size image stored in Facebook using Puppeteer.\nAnd here is a great article how to use Puppeteer to bypass some Captcha sliders.\nStage 4: Error mapping and logging At this stage you will try to map all types of errors that the webpage or API can give into your bot, so it can recognise the difference between an expected and an unexpected error and behave accordantly.\nWhen an unexpected message arrives, my advise is to stop any further actions from being executed, then take some time to look and implement a new behaviour into the bot. This way you avoid having it doing unexpected things or store invalid/incomplete information without letting you know first. A simple email message should be enough to alert you.\nAnother consideration is to handle HTTP status, like 503 (Service Unavailable). Some APIs go to sleep during the night, or during certain hours or days for updates, deployments, maintenance, etc. Remember to keep some kind of log for each request (for example save every script output into a log file,/var/log/crawler1.log ) with timestamp to be able to analyse later the crawler behaviour.\nNormally I also recommend adding some DEBUG flags into your code, so you can enable it if you encounter a weird behaviour, and disable it for normal operation, only having minimal output with a timestamp, so you can know what is the time interval used between the requests and which data was fetched.\nStage 5: Starting it … You reached the point where you can provide some input, for example and identifier (id), and using a command line or other form of interaction you can retrieve information associated to that identifier, and then store it in the database. Now your plan is to automate all this process, mainly trying to disguise the bot’s behaviour as a human behaviour.\nConsiderations before running Probably the major aspect of keeping it running in a loop is the time interval between each request. How many requests are you making per day ? Per hour ? Per minute ? Per second ? You need to have in mind that the more requests you do, the more likely you will be detected. In the other hand, the less requests you do, less data you will have per unit of time, and more time you will need to reach your end goal (if your end goal is retrieve all the information stored on the server).\nHere are some scenarios you can face if you’re caught making to many requests. Their actions can be:\nAdd a limit to the number of requests made per account / ip address. Add captcha (like ReCaptcha) in order to access the information. Block your IP (you will probably easily bypass if you have a dynamic ip, or you can use a proxy or TOR). Remove the service from being online. The first couple of minutes are good to understand the behaviour of the bot. Keeping an eye on your bot logs is a good way to check if the intended behaviour is being applied successfully.\nCreate some statistics of its behaviour:\nTime since last valid request. Generate an alert if is higher than the normal amount. Number of request per hour, per day, per week. Difference between the number of requests of the last couple of days. Stage 6: Keep it running Your major concern is to avoid downtime in your bot due an unexpected behaviour. Here is a list of some actions you should take to better prepare for when an unexpected action is perform on their side.\nDown Notification One of the first things you need to set up is a notification when the website or API is down. This can be useful to understand possible downtime patterns.\nChange Notification After building a bot, you hope that you will do the least amount of maintenance as possible, but with the fast pace of the software development, every few months a new change might occur, like a modification in the HTML code, or the endpoints used. Still, the most difficult part is building it, and after that any change should be easy doable without losing much time, but you will still experience downtime.\nAvoid Detection / Block One way the servers can block you is by your IP address. Normally this is done via a firewall, IDS (Intrusion Detection System) or the service it self, if it has some kind of rate limit capabilities.\nOne of the most common ways to hide your IP is using a proxy service, and TOR offers you a great way to do that. You can specify which country you want to be your exit from the TOR network, if for example the service only accepts IP addresses from certain countries.\nI also recommend changing the TOR exit node IP address by restarting the TOR service several times a day. This is good for two reasons: some nodes are painfully slow and can trigger a timeout in your connection, and to increase the number of IP requested tracked on the server side. But remember that TOR exit nodes can be blocked, o be careful on the amount of request you do to crawl a service before they detect and disable this option.\nStorage You need to save the extracted data in some place. You could save in text files, but probably one of the reasons you are extracting information is to access it later, and maybe perform some analysis to it.\nYou have today a great offer of database engines, from relations databases like MySQL and PostgreSQL to different types of NoSQL databases like MongoDB, ElasticSearch, Redis, etc. You can investigate which one will be better for your use case, of choose the one you’re most familiar with and then see if further improvements need to be made down the line.\nMost of my experience with databases optimisation came from having slow queries when performing data analysis on extracted information. The database grew past the expected size, so new solutions needed to be found to keep the query times lower.\nThis can also happen if you plan to apply artificial intelligence to the data before making the next request. In some cases, depending which input are given to the service/page you can infer what values can be return based on which values you have.\nBellow are some possible actions to take into consideration to speed up data access:\nCache Process data on write, if you have an intensive read bot. Use EXPLAIN on your query if you are using MySQL or PostgreSQL. Check what is having a higher cost. Normally adding an index will speed things up, but also if your query is in a complex string, it might be good to split into two columns that using substring to filter it out. Stage 7: Dashboard / Visual Application You should be happy with everything running perfectly now, but in the end, you are just storing data on some tables in a database. An optional step could be the development of a dashboard to visualise the information gathered, adding some graphs, tables, search fields to query the data and create new queries to perform some analysis on the data you’ve retrieved.\nThis will help you better understand it and can also be integrated with other bots or services in the future to connect the data you already have with other sources.\nThis can be specially helpful if you are gathering media files like images, sounds or videos.\nFinal thoughts There is a lot of valuable information on the internet that should be better protected against crawlers and bots, and other information that should be publicly accessible via an API to be easily fetched and used to create useful applications.\nAs we go further into the future, we tend to share more and more of our lives in social media apps and websites, our location, our food, our home, our mood, our trips. There is nothing wrong if we really want to share this information, but we should be aware of how this data is handled by companies and who have access to this information, and what they can do with it.\nCompanies need to step up the responsibility to protect our data, since the beginning of their product. Now with GDPR in Europe, we are getting more ownership of our data, and what companies do with it. The responsibility of owning data needs to be in the company vision from now on, in order to avoid serious issues in the future, something similar of what we’ve experienced in Cambridge Analytica.\nPeople need to be more conscious on what the share online, setting permissions on who can see it and so on.\nOne thing I’m pretty sure, our lives will be less private in the future.\n","wordCount":"3291","inLanguage":"en","image":"https://speeddragon.github.io/images/1__lwXZvQJ7eF__pI8tI1ernCw.jpeg","datePublished":"2019-12-11T12:59:13.731Z","dateModified":"2019-12-11T12:59:13.731Z","mainEntityOfPage":{"@type":"WebPage","@id":"https://speeddragon.github.io/posts/bots-and-crawlers-the-automation-of-information-gathering-e5afc88fb625/"},"publisher":{"@type":"Organization","name":"SpeedDragon Blog","logo":{"@type":"ImageObject","url":"https://speeddragon.github.io/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://speeddragon.github.io/ accesskey=h title="SpeedDragon Blog (Alt + H)">SpeedDragon Blog</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://speeddragon.github.io/archives/ title=archives><span>archives</span></a></li><li><a href=https://speeddragon.github.io/search/ title="search (Alt + /)" accesskey=/><span>search</span></a></li><li><a href=https://speeddragon.github.io/categories/ title=categories><span>categories</span></a></li><li><a href=https://speeddragon.github.io/tags/ title=tags><span>tags</span></a></li><li><a href=https://github.com/speeddragon title=GitHub><span>GitHub</span>&nbsp;
<svg fill="none" shape-rendering="geometricPrecision" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12"><path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"/><path d="M15 3h6v6"/><path d="M10 14 21 3"/></svg></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class="post-title entry-hint-parent">Bots and Crawlers — The automation of information gathering</h1><div class=post-description>Nowadays, with the fast pacing appearance of services on the Internet, we are faced constantly with new challenges. More commonly we are…</div><div class=post-meta><span title='2019-12-11 12:59:13.731 +0000 UTC'>December 11, 2019</span>&nbsp;·&nbsp;16 min</div></header><figure class=entry-cover><img loading=eager src=https://speeddragon.github.io/images/1__lwXZvQJ7eF__pI8tI1ernCw.jpeg alt></figure><div class=post-content><p>Nowadays, with the fast pacing appearance of services on the Internet, we are faced constantly with new challenges. More commonly we are seeing new types of attacks to companies that hold customer information. As today, this is an asset that keeps increasing in value.</p><p>In this article I will introduce you to my view and experience of gathering information since my early days of programming up until now, how it evolved, some challenges I’ve been faced with, and my view of the future.</p><h3 id=early-days>Early Days<a hidden class=anchor aria-hidden=true href=#early-days>#</a></h3><p>Since my first steps on the Internet, I remember bots being present in the famous IRC clients and chatrooms. I was fascinated by how we could automate actions, pre-program some settings to help us complete some specific action and set up rules and parameters to help us reduce the time needed to perform what would be a manual action. These were my early days, the days I first saw a line of code and was something completely strange for me.</p><h4 id=what-is-a-bot>What is a Bot ?<a hidden class=anchor aria-hidden=true href=#what-is-a-bot>#</a></h4><blockquote><p>An Internet <strong>bot</strong>, also known as web robot, WWW robot or simply <strong>bot</strong>, is a <strong>software</strong> application that runs automated tasks (scripts) over the Internet. Typically, <strong>bots</strong> perform tasks that are both simple and structurally repetitive, at a much higher rate than would be possible for a human alone.</p></blockquote><p><img loading=lazy src=/images/0__QoWAGkiIcMOTI__AB.png></p><p>I remember questioning myself how the dialogs were created. The first time I “created” a dialog I think it was in <a href=https://en.wikipedia.org/wiki/Microsoft_Paint>MS Paint</a>, using a print screen of a window (normally Internet Explorer, because it was the “portal” for accessing the Internet) and started adding buttons and changing text and input fields.</p><p>I didn’t create many dialogs in mIRC, but I was fascinated how to automate a command, that could automatically reply to people messages. Creating a bot to reply to someone if a certain keyword was present was fun and then I started creating more complex actions in order to automate some repetitive tasks.</p><p>When I stopped using mIRC, I remember that people created a lot of complex text games (maybe the predecessor of <a href=https://barafranca.com/>Omerta</a> and other first internet web based games).</p><h4 id=what-is-a-crawler>What is a Crawler ?<a hidden class=anchor aria-hidden=true href=#what-is-a-crawler>#</a></h4><blockquote><p>A <strong>crawler</strong> is a program that visits Web sites and reads their pages and other information in order to create entries for a search engine index. The major search engines on the Web all have such a program, which is also known as a “spider” or a “bot.”</p></blockquote><p>Normally crawlers are used in search engine to fetch information to be displayed on future searches. It used to be only GET requests that were indexed, but <a href=https://webmasters.googleblog.com/2011/11/get-post-and-safely-surfacing-more-of.html>since 2011 that Google Bot</a> also perform POST request to retrieve information, reaching further into the amount of information that can be gathered.</p><h3 id=the-growth-of-information-access-viainternet>The growth of information access via Internet<a hidden class=anchor aria-hidden=true href=#the-growth-of-information-access-viainternet>#</a></h3><p><img loading=lazy src=/images/0__i5uycCarVC1ktTTM.jpg></p><p>I start realising that the Internet was getting bigger and bigger, and a lot of systems were starting to become available to access them through a browser. Then I started to see bots being created to scrape text out of the server’s responses (well, that was how Google started) and people started building applications that gathered information from different systems, ultimately providing a service or an API out of it.</p><p>After a while, companies realised that some actions needed to be taken, in order to check if the request was made by a human or a robot. In the early days, performing <a href=https://en.wikipedia.org/wiki/Optical_character_recognition>OCR</a> on a <a href=https://en.wikipedia.org/wiki/CAPTCHA>CAPTCHA</a> or understand what was inside of an image was pretty hard to do with a common computer, but soon that become an interest for the researchers and a problem easily overcome with the additional computational power available in the following years.</p><p><a href=https://www.google.com/recaptcha/>Google ReCaptcha</a> is probably the most widely used system today to prevent robots to access areas of websites that are meant to be accessed by humans only. There are other solutions, some of them are easy to bypass, and some that are very difficult for both humans and robots to solve it.</p><h3 id=stages-of-developing-an-information-gathering-service>Stages of developing an information gathering service<a hidden class=anchor aria-hidden=true href=#stages-of-developing-an-information-gathering-service>#</a></h3><p>You can split the development of a bot/crawler into several stages. The first one starts with the planning, then you have the process of extracting the information and storage. Then you start to know how the website behaves and learn how to run it reliable 24/7.</p><p>Additional steps can be taken in order to help you visualise or learn more about the information that you are gathering.</p><h3 id=stage-1-information-recognition>Stage 1: Information recognition<a hidden class=anchor aria-hidden=true href=#stage-1-information-recognition>#</a></h3><p><img loading=lazy src=/images/1__AXQBh0Vo3GOdtMYgtW8wiQ.jpeg></p><p>The first stage is to understand what information is available to you. Then you will learn how the website behaves, and pinpoint every detail that can be useful into the process of developing your bot.</p><ol><li>Check every visual information that you can extract (text, images, videos, …).</li><li>Check for hidden information that you can extract. Not all information is visible to the naked eye and sometimes you need to lurk in the source code to catch some extra information hidden in form fields or with CSS.</li><li>Check your browser network tab for calls to an API, that may contain extra information that isn’t displayed in the website. In the olds days client side rendering wasn’t a major standard as it is today, so normally what you saw was what you got. Nowadays REST APIs are everywhere and a lot of information is returned in these calls that aren’t displayed visually to the end user.</li><li>Check if you can differ an empty/invalid response from a not working service response. In some services the response of not having any information to show is the same as the service being down. The best way to recognise this is to <strong>log every request made</strong> and search for patterns in the data stored. This will also reveal possible service schedule maintenance patterns that might occur during the collection process.</li><li>Check for possible visible or invisible automation detection system (like ReCaptcha, CloudFlare, etc). This will help assess how viable it will be to implement a crawler/bot on that page.</li></ol><p><img loading=lazy src=/images/1__hBGzZS4ccYV0Q9nGAMxDxA.png></p><p>At the end of this step, you should have all the necessary information to model your data into tables and columns in your database. Depending how much data you plan to extract, it might be good to have some experience in database optimisation, or to start reading about it.</p><h3 id=stage-2-mimic-browser-behaviour>Stage 2: Mimic browser behaviour<a hidden class=anchor aria-hidden=true href=#stage-2-mimic-browser-behaviour>#</a></h3><p>This could be a painful process depending how many requests exists and how many static/dynamic fields you need to send back for verification / validation purposes. At this stage you should select your favourite programming language and start using an HTTP client to try to mimic the browser behaviour.</p><p>To help you in this hard task, you have two great tools at your disposal: the browser inspect (<a href=https://developers.google.com/web/tools/chrome-devtools>Chrome</a>, <a href=https://developer.mozilla.org/en-US/docs/Tools/about:debugging>Firefox</a> and <a href=https://developer.apple.com/safari/tools/>Safari</a>) and <a href=https://portswigger.net/burp/communitydownload>Burp Suite Community Edition</a>.</p><p>You should start by removing the headers parameters until you have the necessary minimum data to send in order to obtain the right response, but sometimes you can’t change anything. It might be picky and that can drive you insane until you have right information to be sent, especially if the service changes its behaviour.</p><p><strong>Accept-Language</strong>: pt-PT,pt;q=0.8,en;q=0.5,en-US;q=0.3<br><strong>Accept</strong>: text/html,application/xhtml+xml,application/xml; q=0.9,*/*; q=0.8</p><h4 id=browser-inspect><strong>Browser Inspect</strong><a hidden class=anchor aria-hidden=true href=#browser-inspect>#</a></h4><p><img loading=lazy src=/images/1__5fcZeIAfaeIQ9trHjIghiA.png></p><p>When you start analysing the network requests, remember that you should open the network tab before the page is loaded, or if it was already loaded, make sure you refresh it so you can catch all network requests made inside that tab.</p><p>Inside the browser inspection tool, you will see which data is sent and received between your browser and the server, which websites it connects to, cookies, parameters sent, etc. Normally you want to find the request that have the information you want to extract (your goal), and them backtrack all the necessary steps from there, like for example search for early requests being made that must be replicated in your crawler.</p><p>In the majority of the cases you just need to know which request type is it (GET or POST, the most common ones) and which endpoint you are connecting to (URL), as well which parameters you need to input to extract the result returned by the server. Typically, this could be a <code>form-data</code> format like <code>key=value&amp;key2=value2</code> or a JSON format string input <code>{"key": "value", "key2": "value2"}</code>.</p><p>If this doesn’t work, the next step is to investigate the cookies, and check if you need to do a previous request in order to generate the necessary cookies for the server to send the reply you are expecting. Sometimes this could be a long and painfully processes, so another way to do this is to export the request that return the information you want into a cURL command (on Chrome, right click in the request you want and select <code>Copy > Copy as cURL</code>), and check in your command line if that works. Then you start to remove what you think might be unnecessary data until the response isn’t the same. That’s how you find out what fields are required or not.</p><h4 id=burp-suite-community-edition><strong>Burp Suite Community Edition</strong><a hidden class=anchor aria-hidden=true href=#burp-suite-community-edition>#</a></h4><p><img loading=lazy src=/images/0__F6jdBHHwhowwEw3F.png></p><p><a href=https://portswigger.net/burp/communitydownload>Burp</a> is probably the best tool to help you understand what information is being sent from your browser into the server, using a proxy.</p><p>This is a powerful tool used by hackers, security researches, and other IT personal, in order to easily access all the requests being sent by your browser, as well as utilise the handful of features that allows them to tamper with requests and resend them, in order to better understand the server’s behaviour.</p><h3 id=stage-3-information-extraction-automation>Stage 3: Information extraction automation<a hidden class=anchor aria-hidden=true href=#stage-3-information-extraction-automation>#</a></h3><p>After you have identified the data to extract and have it in your side (replicating the browser behaviour), the next step is to store it into your database.</p><ol><li>Don’t forget to sanitise all information retrieved from the website / API. You shouldn’t trust the information that you extracted, and you should handle it as “user input”. If you display it in a HTML page, convert all input into <a href=https://www.w3schools.com/html/html_entities.asp>html entities</a>. Don’t ever concatenate extracted data into a SQL query, use prepare statements. Try to follow security standards like a normal web application.</li><li>Use a <a href=https://www.codementor.io/wang90925/top-10-best-usage-examples-of-php-simple-html-dom-parser-i8ik76e16>DOM parser</a> with query selectors to extract the information if the response is in HTML format. If the response is in JSON format use the decoder available for your language. For example, in PHP you have <code>json_decode($input)</code> .</li></ol><p><img loading=lazy src=/images/1__IYwHpsNIlsIuh9YVmIORig.jpeg></p><p>There might be some cases, like Facebook where parsing HTML as an HTTP request might be difficult, because everything is rendered on the client side with complex/obfuscated JavaScript code. Since the Cambridge Analytic scandal, Facebook has been locking access to its GraphQL API data, making it harder and harder to get “clean” data from it.</p><p>The solution for this issues is to use <a href=https://github.com/GoogleChrome/puppeteer>Puppeteer</a>, a API to the Google Chrome browser that allows you to control it using NodeJS. Its behaviour is similar as doing an HTTP request, but with much more functionality because of the JavaScript being executed in the client side. Absolutely worthwhile looking into it.</p><p>The following code is a NodeJS example of how to extract the biggest size image stored in Facebook using Puppeteer.</p><p>And <a href=https://medium.com/@filipvitas/how-to-bypass-slider-captcha-with-js-and-puppeteer-cd5e28105e3c>here</a> is a great article how to use Puppeteer to bypass some Captcha sliders.</p><h3 id=stage-4-error-mapping-andlogging>Stage 4: Error mapping and logging<a hidden class=anchor aria-hidden=true href=#stage-4-error-mapping-andlogging>#</a></h3><p>At this stage you will try to map all types of errors that the webpage or API can give into your bot, so it can recognise the difference between an expected and an unexpected error and behave accordantly.</p><p>When an unexpected message arrives, my advise is to stop any further actions from being executed, then take some time to look and implement a new behaviour into the bot. This way you avoid having it doing unexpected things or store invalid/incomplete information without letting you know first. A simple email message should be enough to alert you.</p><p>Another consideration is to handle HTTP status, like 503 (Service Unavailable). Some APIs go to sleep during the night, or during certain hours or days for updates, deployments, maintenance, etc. Remember to keep some kind of log for each request (for example save every script output into a log file,<code>/var/log/crawler1.log</code> ) with <strong>timestamp</strong> to be able to analyse later the crawler behaviour.</p><p>Normally I also recommend adding some <code>DEBUG</code> flags into your code, so you can enable it if you encounter a weird behaviour, and disable it for normal operation, only having minimal output with a timestamp, so you can know what is the time interval used between the requests and which data was fetched.</p><h3 id=stage-5-starting-it>Stage 5: Starting it …<a hidden class=anchor aria-hidden=true href=#stage-5-starting-it>#</a></h3><p>You reached the point where you can provide some input, for example and identifier (id), and using a command line or other form of interaction you can retrieve information associated to that identifier, and then store it in the database. Now your plan is to automate all this process, mainly trying to disguise the bot’s behaviour as a human behaviour.</p><h4 id=considerations-beforerunning>Considerations before running<a hidden class=anchor aria-hidden=true href=#considerations-beforerunning>#</a></h4><p>Probably the major aspect of keeping it running in a loop is the time interval between each request. How many requests are you making per day ? Per hour ? Per minute ? Per second ? You need to have in mind that the more requests you do, the more likely you will be detected. In the other hand, the less requests you do, less data you will have per unit of time, and more time you will need to reach your end goal (if your end goal is retrieve all the information stored on the server).</p><p>Here are some scenarios you can face if you’re caught making to many requests. Their actions can be:</p><ul><li>Add a limit to the number of requests made per account / ip address.</li><li>Add captcha (like ReCaptcha) in order to access the information.</li><li>Block your IP (you will probably easily bypass if you have a dynamic ip, or you can use a proxy or <a href=https://www.torproject.org/download/>TOR</a>).</li><li>Remove the service from being online.</li></ul><p>The first couple of minutes are good to understand the behaviour of the bot. Keeping an eye on your bot logs is a good way to check if the intended behaviour is being applied successfully.</p><p>Create some statistics of its behaviour:</p><ul><li>Time since last valid request. Generate an alert if is higher than the normal amount.</li><li>Number of request per hour, per day, per week.</li><li>Difference between the number of requests of the last couple of days.</li></ul><h3 id=stage-6-keep-itrunning>Stage 6: Keep it running<a hidden class=anchor aria-hidden=true href=#stage-6-keep-itrunning>#</a></h3><p>Your major concern is to avoid downtime in your bot due an unexpected behaviour. Here is a list of some actions you should take to better prepare for when an unexpected action is perform on their side.</p><h4 id=down-notification>Down Notification<a hidden class=anchor aria-hidden=true href=#down-notification>#</a></h4><p>One of the first things you need to set up is a notification when the website or API is down. This can be useful to understand possible downtime patterns.</p><h4 id=change-notification>Change Notification<a hidden class=anchor aria-hidden=true href=#change-notification>#</a></h4><p>After building a bot, you hope that you will do the least amount of maintenance as possible, but with the fast pace of the software development, every few months a new change might occur, like a modification in the HTML code, or the endpoints used. Still, the most difficult part is building it, and after that any change should be easy doable without losing much time, but you will still experience downtime.</p><h4 id=avoid-detection-block>Avoid Detection / Block<a hidden class=anchor aria-hidden=true href=#avoid-detection-block>#</a></h4><p>One way the servers can block you is by your IP address. Normally this is done via a firewall, IDS (Intrusion Detection System) or the service it self, if it has some kind of rate limit capabilities.</p><p>One of the most common ways to hide your IP is using a proxy service, and <a href=https://www.torproject.org/>TOR</a> offers you a great way to do that. You can specify which country you want to be your exit from the TOR network, if for example the service only accepts IP addresses from certain countries.</p><p>I also recommend changing the TOR exit node IP address by restarting the TOR service several times a day. This is good for two reasons: some nodes are painfully slow and can trigger a timeout in your connection, and to increase the number of IP requested tracked on the server side. But remember that TOR exit nodes <a href=https://stackoverflow.com/questions/9780038/is-it-possible-to-block-tor-users>can be blocked</a>, o be careful on the amount of request you do to crawl a service before they detect and disable this option.</p><h4 id=storage>Storage<a hidden class=anchor aria-hidden=true href=#storage>#</a></h4><p>You need to save the extracted data in some place. You could save in text files, but probably one of the reasons you are extracting information is to access it later, and maybe perform some analysis to it.</p><p>You have today a great offer of database engines, from relations databases like MySQL and PostgreSQL to different types of NoSQL databases like MongoDB, ElasticSearch, Redis, etc. You can investigate which one will be better for your use case, of choose the one you’re most familiar with and then see if further improvements need to be made down the line.</p><p>Most of my experience with databases optimisation came from having slow queries when performing data analysis on extracted information. The database grew past the expected size, so new solutions needed to be found to keep the query times lower.</p><p>This can also happen if you plan to apply artificial intelligence to the data before making the next request. In some cases, depending which input are given to the service/page you can infer what values can be return based on which values you have.</p><p>Bellow are some possible actions to take into consideration to speed up data access:</p><ul><li>Cache</li><li>Process data on write, if you have an intensive read bot.</li><li>Use <a href=http://www.postgresqltutorial.com/postgresql-explain/>EXPLAIN</a> on your query if you are using MySQL or PostgreSQL. Check what is having a higher cost. Normally adding an index will speed things up, but also if your query is in a complex string, it might be good to split into two columns that using substring to filter it out.</li></ul><h3 id=stage-7-dashboard--visual-application>Stage 7: Dashboard / Visual Application<a hidden class=anchor aria-hidden=true href=#stage-7-dashboard--visual-application>#</a></h3><p><img loading=lazy src=/images/1__Y__K4bpH1IXEua6hcshfESw.jpeg></p><p>You should be happy with everything running perfectly now, but in the end, you are just storing data on some tables in a database. An optional step could be the development of a dashboard to visualise the information gathered, adding some graphs, tables, search fields to query the data and create new queries to perform some analysis on the data you’ve retrieved.</p><p>This will help you better understand it and can also be integrated with other bots or services in the future to connect the data you already have with other sources.</p><p>This can be specially helpful if you are gathering media files like images, sounds or videos.</p><h3 id=final-thoughts>Final thoughts<a hidden class=anchor aria-hidden=true href=#final-thoughts>#</a></h3><p>There is a lot of valuable information on the internet that should be better protected against crawlers and bots, and other information that should be publicly accessible via an API to be easily fetched and used to create useful applications.</p><p>As we go further into the future, we tend to share more and more of our lives in social media apps and websites, our location, our food, our home, our mood, our trips. There is nothing wrong if we really want to share this information, but we should be aware of how this data is handled by companies and who have access to this information, and what they can do with it.</p><p>Companies need to step up the responsibility to protect our data, since the beginning of their product. Now with <a href=https://eugdpr.org/>GDPR</a> in Europe, we are getting more ownership of our data, and what companies do with it. The responsibility of owning data needs to be in the company vision from now on, in order to avoid serious issues in the future, something similar of what we’ve experienced in Cambridge Analytica.</p><p>People need to be more conscious on what the share online, setting permissions on who can see it and so on.</p><p>One thing I’m pretty sure, our lives will be less private in the future.</p><p><img loading=lazy src=/images/1__a3HSKaQ6l__LftoqjTUYscQ.jpeg></p></div><footer class=post-footer><ul class=post-tags></ul><nav class=paginav><a class=prev href=https://speeddragon.github.io/posts/coverage-with-partition-test-a7835a147b42/><span class=title>« Prev</span><br><span>Elixir code coverage with partition test</span>
</a><a class=next href=https://speeddragon.github.io/posts/@speeddragon/phoenix-with-image-upload-to-s3-in-an-api-implementation-and-testing-6ab5187175b0/><span class=title>Next »</span><br><span>Phoenix with image upload to S3 in an API: Implementation and testing</span></a></nav><ul class=share-buttons><li><a target=_blank rel="noopener noreferrer" aria-label="share Bots and Crawlers — The automation of information gathering on x" href="https://x.com/intent/tweet/?text=Bots%20and%20Crawlers%e2%80%8a%e2%80%94%e2%80%8aThe%20automation%20of%20information%20gathering&amp;url=https%3a%2f%2fspeeddragon.github.io%2fposts%2fbots-and-crawlers-the-automation-of-information-gathering-e5afc88fb625%2f&amp;hashtags="><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446C483.971.0 512 28.03 512 62.554zM269.951 190.75 182.567 75.216H56L207.216 272.95 63.9 436.783h61.366L235.9 310.383l96.667 126.4H456L298.367 228.367l134-153.151H371.033zM127.633 110h36.468l219.38 290.065H349.5z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Bots and Crawlers — The automation of information gathering on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fspeeddragon.github.io%2fposts%2fbots-and-crawlers-the-automation-of-information-gathering-e5afc88fb625%2f&amp;title=Bots%20and%20Crawlers%e2%80%8a%e2%80%94%e2%80%8aThe%20automation%20of%20information%20gathering&amp;summary=Bots%20and%20Crawlers%e2%80%8a%e2%80%94%e2%80%8aThe%20automation%20of%20information%20gathering&amp;source=https%3a%2f%2fspeeddragon.github.io%2fposts%2fbots-and-crawlers-the-automation-of-information-gathering-e5afc88fb625%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Bots and Crawlers — The automation of information gathering on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fspeeddragon.github.io%2fposts%2fbots-and-crawlers-the-automation-of-information-gathering-e5afc88fb625%2f&title=Bots%20and%20Crawlers%e2%80%8a%e2%80%94%e2%80%8aThe%20automation%20of%20information%20gathering"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Bots and Crawlers — The automation of information gathering on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fspeeddragon.github.io%2fposts%2fbots-and-crawlers-the-automation-of-information-gathering-e5afc88fb625%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Bots and Crawlers — The automation of information gathering on whatsapp" href="https://api.whatsapp.com/send?text=Bots%20and%20Crawlers%e2%80%8a%e2%80%94%e2%80%8aThe%20automation%20of%20information%20gathering%20-%20https%3a%2f%2fspeeddragon.github.io%2fposts%2fbots-and-crawlers-the-automation-of-information-gathering-e5afc88fb625%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Bots and Crawlers — The automation of information gathering on telegram" href="https://telegram.me/share/url?text=Bots%20and%20Crawlers%e2%80%8a%e2%80%94%e2%80%8aThe%20automation%20of%20information%20gathering&amp;url=https%3a%2f%2fspeeddragon.github.io%2fposts%2fbots-and-crawlers-the-automation-of-information-gathering-e5afc88fb625%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentColor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Bots and Crawlers — The automation of information gathering on ycombinator" href="https://news.ycombinator.com/submitlink?t=Bots%20and%20Crawlers%e2%80%8a%e2%80%94%e2%80%8aThe%20automation%20of%20information%20gathering&u=https%3a%2f%2fspeeddragon.github.io%2fposts%2fbots-and-crawlers-the-automation-of-information-gathering-e5afc88fb625%2f"><svg width="30" height="30" viewBox="0 0 512 512" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446zM183.8767 87.9921h-62.034L230.6673 292.4508V424.0079h50.6655V292.4508L390.1575 87.9921H328.1233L256 238.2489z"/></svg></a></li></ul></footer></article></main><footer class=footer><span>&copy; 2025 <a href=https://speeddragon.github.io/>SpeedDragon Blog</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>
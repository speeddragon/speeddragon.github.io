[{"content":"Disclaimer: I don\u0026rsquo;t have a degree in electronic engineering. My knowledge comes from reading and experimenting. You should do your own research before trying to replicate my work.\nThe problem Old vehicles start to have issues as they age, and a common one with Nissan 200SX (S13) (240SX in America and 180SX in Japan) is the cluster that stops working, either the speedometer (more common) or the tachometer (engine RPM).\nThe issue with the speedometer is the needle that either stops working or jumps to the other end of the scale (240+ km/h). This is associated with broken solder joints (mainly X1, X2, Y1, Y2) in the speedometer PCB, and usually a reflow (resoldering) is enough to fix.\nI had asked a friend to do this for me, and it fixed the needle moving over 240 km/h issue, but created a new one, where the needle didn\u0026rsquo;t move after reaching 82 km/h. Why 82 km/h? This took me a long time to understand. Inside the round metal shell, there are 2 coils wrapped in a cross (X) shape. This cross is tilted 45 degrees angle, and the upper limit on this 45 degree angle is 82 km/h.\nSo only one axis was being driven, and the other (from 82 to 270+ km/h) wasn\u0026rsquo;t able to move the needle. A person even reported only starting to work after 80 km/h, but being erratic after that. That would match the case of only the other axis driving the needle.\nSince then, I\u0026rsquo;ve improved my soldering skills, so I decided to attempt to fix it by myself, but before doing it, I wanted to come up with a test on the lab to check if it was working properly, before installing it in the car and driving at more than 82 km/h to see if the issue was fixed.\nSince I also had an issue with the tachometer, I also want to simulate this one in the lab to understand what the issue could be.\nTachometer Simulation The first one I\u0026rsquo;ve tried to simulate was the tachometer. This uses a DC pulse signal from the CAS (Crank Angle Sensor) that happens 2 times per revolution.\nrotations per minute = pulse frequency / 2 * 60 seconds The easy way to do this would be buying a frequency generator, like this. But I didn\u0026rsquo;t have one, and I also wanted to know how I could change the needle position on a real car gauge, so this was the perfect time to learn how to do it.\nTo simulate this with ESP32 (and with Arduino IDE) I need to send a 12-volt PWM signal at 50% duty cycle. If you don\u0026rsquo;t know about duty cycle (I didn\u0026rsquo;t), it is the amount of time the signal is high. In this case, 50% of the time the signal is high and 50% is low.\nThe first thing is to choose a PWM pin. The ESP32 has several PWM pins, so I used the GPIO 4 (D4).\nThe second thing is to transform the 3.3 volts PWM signal to 12 volts. For this I\u0026rsquo;ve used both BJT transistor and a MOSFET, but for this use case a BJT is enough, since it doesn\u0026rsquo;t have too much current (under the 600 mA mention in the 2N2222 datasheet).\nAlthough the IRLZ44N works with 3.3v from the ESP32, it will not be fully open, and MOSFETs aren\u0026rsquo;t meant to be in a transition state, becoming inefficient and generating more heat.\nI decided to use 2N2222 for the BJT transistor (2N3904 would have probably been enough max current to be used as well) to drive the IRFZ44N.\nThe tachometer has 3 pins:\nIGN (ignition, V12+), GND (ground) SIG (signal). I was having some issues understanding how I could send the signal to the tachometer, but this example helped me out.\nHere is the schematics:\nComponents:\nStep-down LM2596 board ESP32 WROOM Devkit 2N2222 IRFZ44N 1kΩ resistor 10kΩ resistor Breadboard and jumper wires (optional, but useful) The code sets 3 RPM values (3000, 6000 and 9000) spaced by 1 second from each other.\n// Define the pin you want to use for PWM int pwmPin = 4; // Example: Using GPIO 4 int ledPin = 2; // Define the PWM channel and frequency int channel = 0; int baseFrequency = 100; // 100 Hz int resolution = 8; // bit resolution void setup() { pinMode(ledPin, OUTPUT); digitalWrite(ledPin, HIGH); // Attach the pin to the channel and set frequency ledcAttach(pwmPin, baseFrequency, resolution); } void loop() { digitalWrite(ledPin, HIGH); // 3000 rpm ledcWriteTone(pwmPin, 100); delay(1000); // 6000 rpm ledcWriteTone(pwmPin, 200); delay(1000); // 9000 rpm ledcWriteTone(pwmPin, 300); digitalWrite(ledPin, LOW); delay(1000); } In this example, I also control the LED pin to know when the ESP32 is running the code. I had some issues with the breadboard I was using, causing the ESP32 to reboot frequently, not being able to understand why I didn\u0026rsquo;t get the PWM values when running via 12 volts, but working properly when using USB.\nEverything worked as expected, but I couldn\u0026rsquo;t replicate the same behaviour in another RPM gauge from a Renault 19. I believe that in some cars, they expect a higher signal from the distributor.\nSpeedometer Simulation The speedometer works in a different way than the tachometer. In this vehicle, it uses a 2\u0026ndash;wire reluctor speed sensor to provide speed information. This sensor generates an AC signal to the cluster that translates into a Hall effect signal to the ECU.\nThe AC signal changes in amplitude and frequency as the speed increases.\nTo simulate an AC signal with DC, I started to investigate what the options were, but the simplest would be to use a capacitor and a resistor in series to shift the DC square wave signal from the PWM (generated between 0 and 12 volts) to an AC square wave signal (between -6 and 6 volts).\nI used the previous schematic and the base for this one, and added the new components. The new AC signal was 4.7 volts, and I\u0026rsquo;ve used the previous frequency but added a new step by setting the frequency to 400 Hz. After uploading the code, the needle was moving between 0, 30, and 60 km/h, but didn\u0026rsquo;t reach the 90 km/h, which was the value I wanted, to replicate the \u0026ldquo;bug\u0026rdquo; of not going past 82 km/h.\nThe issue was regarding the voltage not being high enough for the frequency I was using, so I modified the resistor value before the conversion to the AC signal to allow more current to flow. Modifying the 10k ohms to 4.7k ohms was enough, and now the signal was 5.6 volts, enough to reach over 82 km/h.\nFixing speedometer I\u0026rsquo;ve tried to resolder X1, X2, Y1, and Y2, but it took me a while to fix it. Not sure if the reason was that, or this time I also soldered the IC1 pin (top row, 2 on the left side) that is connected to Y2, which was lacking some solder.\nVideo with the speedometer fixed.\nBut between attempts, I took some time to reverse engineer the device and create the schematics for the speedometer. It was my first time doing schematics and using KiCad, so there were some issues moving things around, and I might have misunderstood some traces. It is also missing some resistor values, for which I didn\u0026rsquo;t take the time to read the band colors.\nFeel free to submit updates about the schematics in the GitHub repo.\nFor the small capacitor values, I used the ones provided by Shaker783396.\nConclusion It took me 2 weeks to finally fix it, but this was a fun project to take on. All the hours looking into Rossmann repairing Apple Macbooks motherboards made useful now.\nOne cool thing would be to replace this module in the cluster with a custom one that uses more modern electronic and stepper motors instead of coils. There is a guy doing that for the Mazda RX7 (FD). This forum also has a good thread of common issues with these clusters.\n","permalink":"https://speeddragon.github.io/posts/vehicle-cluster-simulation-and-fix/","summary":"\u003cp\u003e\u003cstrong\u003eDisclaimer:\u003c/strong\u003e I don\u0026rsquo;t have a degree in electronic engineering. My knowledge comes from reading and experimenting. You should do your own research before trying to replicate my work.\u003c/p\u003e\n\u003ch1 id=\"the-problem\"\u003eThe problem\u003c/h1\u003e\n\u003cp\u003eOld vehicles start to have issues as they age, and a common one with \u003ca href=\"https://en.wikipedia.org/wiki/Nissan_Silvia#S13\"\u003eNissan 200SX (S13)\u003c/a\u003e (240SX in America and 180SX in Japan) is the cluster that stops working, either the speedometer (more common) or the tachometer (engine RPM).\u003c/p\u003e\n\u003cp\u003eThe issue with the speedometer is the needle that either stops working or jumps to the other end of the scale (240+ km/h). This is associated with broken solder joints (mainly X1, X2, Y1, Y2) in the speedometer PCB, and usually a reflow (resoldering) is enough to fix.\u003c/p\u003e","title":"Vehicle cluster simulation"},{"content":"In one of my personal projects I\u0026rsquo;ve implemented a GenServer to store slow queries. One of these queries was returning 17+s seconds to return a result, so I decided to investigate the reason.\nFor the context, I\u0026rsquo;m using a Postgres database and use PgAdmin as UI to test some queries. The first step was to run the query on PgAdmin, but got a response immediately. Shouldn\u0026rsquo;t this query take 17+ seconds?\nFor a long time, I didn\u0026rsquo;t take a further deep dive into this because it wasn\u0026rsquo;t impacting the rest of the application that bad, so I just wrote a quick GitHub issue with the details to be checked later. At the time, a quick google search didn\u0026rsquo;t produce any results.\nHow to replicate this After 3 years, I decided to check this, now with more time to look into it. My first thought was to mimic the application behaviour and use prepared statements.\nThe query was something like this:\nSELECT i0.\u0026#34;id\u0026#34;, i0.\u0026#34;field1\u0026#34; FROM \u0026#34;table1\u0026#34; AS i0 WHERE (((i0.\u0026#34;field1\u0026#34; LIKE $1) AND (i0.\u0026#34;field2\u0026#34; = $2)) AND ((i0.\u0026#34;field3\u0026#34; = 3) AND (i0.\u0026#34;field4\u0026#34; \u0026gt;= $3))) AND (i0.\u0026#34;id\u0026#34; \u0026gt; $4) ORDER BY i0.\u0026#34;id\u0026#34; LIMIT 1 I can create a prepare statement by append PREPARE query1 (text, text, text, int) AS to the beginning of the query. Then to execute I can call:\nEXPLAIN (ANALYZE, BUFFERS) EXECUTE q3(\u0026#39;input1%\u0026#39;, \u0026#39;input2\u0026#39;, \u0026#39;input3\u0026#39;, 11200000); To delete or modify the query, I need to delete it first, DEALLOCATE query1;\nThen, by running the query above more than 5 times, the 6th time would take the 17+ seconds instead of the sub second reply. Now I can try to understand the reason, since the query plan is provided.\nThe first five times runs in less than 5 ms with the following query plan:\nLimit (cost=0.43..7792.39 rows=1 width=76) (actual time=5.224..5.226 rows=1 loops=1) Buffers: shared hit=1104 read=81 dirtied=1 -\u0026gt; Index Scan using table1_pkey on table1 i0 (cost=0.43..132463.71 rows=17 width=76) (actual time=5.222..5.223 rows=1 loops=1) Index Cond: (id \u0026gt; 11200000) Filter: (((field1)::text ~~ \u0026#39;input%\u0026#39;::text) AND ((field2)::text \u0026gt;= \u0026#39;input2\u0026#39;::text) AND ((field3)::text = \u0026#39;input3\u0026#39;::text) AND (field4 = 3)) Rows Removed by Filter: 1577 Buffers: shared hit=1104 read=81 dirtied=1 Planning Time: 0.440 ms Execution Time: 5.259 ms The sixth time, it took 21 seconds with the following query plan.\nLimit (cost=4839.18..4839.18 rows=1 width=76) (actual time=21123.561..21123.567 rows=1 loops=1) Buffers: shared hit=4764 read=131142 dirtied=1612 written=10 -\u0026gt; Sort (cost=4839.18..4839.19 rows=5 width=76) (actual time=21123.558..21123.562 rows=1 loops=1) Sort Key: id Sort Method: top-N heapsort Memory: 25kB Buffers: shared hit=4764 read=131142 dirtied=1612 written=10 -\u0026gt; Bitmap Heap Scan on table1 i0 (cost=4259.79..4839.16 rows=5 width=76) (actual time=3705.783..21123.033 rows=365 loops=1) Recheck Cond: (((field1)::text ~~ $1) AND ((field3)::text = $2)) Rows Removed by Index Recheck: 3645131 Filter: (((plate_letters)::text \u0026gt;= $3) AND (id \u0026gt; $4) AND (field4 = 3)) Rows Removed by Filter: 471691 Heap Blocks: exact=45202 lossy=66193 Buffers: shared hit=4761 read=131142 dirtied=1612 written=10 -\u0026gt; BitmapAnd (cost=4259.79..4259.79 rows=147 width=0) (actual time=3366.735..3366.737 rows=0 loops=1) Buffers: shared hit=4751 read=19757 written=10 -\u0026gt; Bitmap Index Scan on table1_field1_trgm_idx (cost=0.00..1798.89 rows=55318 width=0) (actual time=774.775..774.775 rows=480650 loops=1) Index Cond: ((field1)::text ~~ $1) Buffers: shared hit=4750 read=3405 written=7 -\u0026gt; Bitmap Index Scan on table1_field3_idx (cost=0.00..2460.65 rows=29346 width=0) (actual time=2581.453..2581.454 rows=559635 loops=1) Index Cond: ((field3)::text = $2) Buffers: shared hit=1 read=16352 written=3 Planning Time: 0.229 ms Execution Time: 21124.522 ms Solution Since I can replicate our problem now, I can understand it and find the solution for it.\nTo achieve a quick result I can for the cache plan to use force_custom_plan.\nSET plan_cache_mode = force_custom_plan; I didn\u0026rsquo;t use force_custom_plan, so I tweak the query so I could achieve similar result without the side effect of being too slow by requesting some filters that were too broad.\nArtificial Inteligance (or LLM) First time I\u0026rsquo;ve detected this was August 2022. ChatGPT was first launch in November 2022.\nI\u0026rsquo;ve been trying to integrate usage of LLM into my work for some time, but I didn\u0026rsquo;t like the results. When too specific about what I need it produce wrong results, but I\u0026rsquo;ve changed my approach to search for more simple stuff.\nAfter I\u0026rsquo;ve solved this, I decided to try to solve it using ChatGPT. Here it answer.\nI\u0026rsquo;ve asked to show some solutions how to fix the planning, since it was changing and the first one was to set force_custom_plan. I ended up using the 4th, rewriting the query.\n","permalink":"https://speeddragon.github.io/posts/postgres-slow-query/","summary":"\u003cp\u003eIn one of my personal projects I\u0026rsquo;ve implemented a \u003ccode\u003eGenServer\u003c/code\u003e to store slow queries. One of\nthese queries was returning 17+s seconds to return a result, so I decided to investigate the reason.\u003c/p\u003e\n\u003cp\u003eFor the context, I\u0026rsquo;m using a Postgres database and use PgAdmin as UI to test some queries.\nThe first step was to run the query on PgAdmin, but got a response immediately. Shouldn\u0026rsquo;t this query take 17+ seconds?\u003c/p\u003e","title":"Postgres - I can't replicate a slow query"},{"content":"Intro The people that creates or uses crawlers to fetch information from a web page, know that you can lose access if the website owner add a WAF (Web Application Firewall) to the web page. This will certainly make your life more difficult.\nSome examples of WAF are:\nCloudFlare WAF Imperva Cloud WAF AWS/Azure WAF How it work? https://pt-corp.storage.yandexcloud.net/upload/corporate/ww-en/download/PT-devteev-CC-WAF-ENG.pdf Normally sits between the client and the server. Monitor and filters HTTP traffic between server and clients. Contains complex rules to detect malicious traffic. SQL Injections; XSS; TOR; Denial of Service (DoS), etc. Blocks malicious traffic. Story In one of my side projects I maintain some crawlers. Recently one stopped working after the website changed, but that wasn\u0026rsquo;t the only reason for the crawler to stop working, they also added a WAF to it.\nMy first reaction was to try to find a way to bypass it, so I\u0026rsquo;ve searched for a bit but I didn\u0026rsquo;t find anything relevant.\nNormally there are some options to bypass a WAF depending on what you\u0026rsquo;re doing:\nHTTP Parameter Pollution HTTP Parameter Fragmentation nullbyte replacement Since the URL wasn\u0026rsquo;t the same, I though maybe there was a way to access to the previous website? And maybe that isn\u0026rsquo;t protected by a WAF? The first thing that come to mind was to find the “origin” IP Address.\nSo the question was, is there a service that kept a record of different IP? Yes there is!\nI\u0026rsquo;ve found Security Trails, a website that allow us to see historical data of a domain. My idea was if I can access directly to the IP address, maybe I could bypass the WAF, of maybe they still kept the old server running without the WAF.\nWith the old IP address, I confirmed that the old service is still working and being updated, so I updated the configuration on crawler to use the IP address instead of the domain, but added the domain under Host header so the server could redirect to the right host.\ncurl --header \u0026#39;Host: jn.pt\u0026#39; http://148.69.168.38/ Now, the request that was being intercepted when requesting via TOR is able to request as previously.\nWe must be aware that since this is a deprecated service, it can be removed anytime and we need to fallback to the new service (and find a way to use it for this purpose).\nConclusion How this can happen? Mostly on companies that aren\u0026rsquo;t tech aren\u0026rsquo;t their core, they forget to shutdown the old services or will take some time to decommission it.\n","permalink":"https://speeddragon.github.io/posts/bypass_waf/","summary":"\u003ch1 id=\"intro\"\u003eIntro\u003c/h1\u003e\n\u003cp\u003eThe people that creates or uses crawlers to fetch information from a web page, know that you can lose access if the website owner add a WAF (Web Application Firewall) to the web page. This will certainly make your life more difficult.\u003c/p\u003e\n\u003cp\u003eSome examples of WAF are:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eCloudFlare WAF\u003c/li\u003e\n\u003cli\u003eImperva Cloud WAF\u003c/li\u003e\n\u003cli\u003eAWS/Azure WAF\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch1 id=\"how-it-work\"\u003eHow it work?\u003c/h1\u003e\n\u003cp\u003e\u003cimg alt=\"WAF example\" loading=\"lazy\" src=\"/images/waf_example.jpg#center\"\u003e\u003c/p\u003e\n\u003cp class=\"image-description\"\u003e\nhttps://pt-corp.storage.yandexcloud.net/upload/corporate/ww-en/download/PT-devteev-CC-WAF-ENG.pdf\n\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eNormally sits between the client and the server.\u003c/li\u003e\n\u003cli\u003eMonitor and filters HTTP traffic between server and clients.\u003c/li\u003e\n\u003cli\u003eContains complex rules to detect malicious traffic.\n\u003cul\u003e\n\u003cli\u003eSQL Injections; XSS; TOR; Denial of Service (DoS), etc.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eBlocks malicious traffic.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch1 id=\"story\"\u003eStory\u003c/h1\u003e\n\u003cp\u003eIn one of my side projects I maintain some crawlers. Recently one stopped working after the website changed, but that wasn\u0026rsquo;t the only reason for the crawler to stop working, they also added a WAF to it.\u003c/p\u003e","title":"Bypass WAF"},{"content":"On a recent trip to the United States, I noticed there is a way to skip the TSA queue waiting for clearing immigration going out of the country, so I decided to share it here. But first I want to tell you how I ended up finding it.\nI arrived at the airport and went to the TSA queue for my designated flight gate. Waited until it was my time to go through with my passport and online boarding pass. While my friend went without any issues, I didn’t have the same result. The TSA officer told me to go back to the airline check-in counter and ask them to reprint my boarding ticket. He also gave me this little paper.\nPaper with the TSA request (sorry for the finger, added the missing words) Although I cannot be 100% sure, I think this happened because the name on my boarding pass is different from the one on my passport since the airline has a character limit and my full name didn’t fit, hence the need for an identity validation from the airline person.\nAlso, while doing the online check-in on the way to the USA, I couldn’t get the boarding pass (although my friend who was under the same reservation could), not sure if the problem was related or not. I tried to ask at the airline counter but they didn’t know the reason and only printed my boarding pass.\nBack to the VID check, the ticket explained that my identity needed to be validated by the airline and that a new boarding pass needed to be issued. Also states that to return to the TSA front line. I’m not sure if all TSA counters and queues (there were several in the airport I was in) had this “fast lane” queue, but on the airline check-in, they directed me to a specific one that had a lot of people waiting in the normal queues.\nAfter going through the airline counter and getting my physical boarding pass (which they stated my friend also needed to get to board the airplane), I went to the TSA agent organizing the people at the end of the queue that allowed me to get into the fast lane with only 2 people ahead of me and the rest of the process was normal.\nAnd that was it. But since I’m geeky about this stuff, I wanted to know what is the difference between the information on the online boarding pass and the physical one. Both have the PDF417 bar code with the same information except the 2 fields, one with some small changes that I cannot understand their meaning and another with some kind of signature hash (probably base64).\nPhoto by CardMapr.nl on Unsplash The first field had the values O4041B (boarding pass without signature) and W4040B (boarding pass with signature) and the field with the following signature.\n^460MEQCIBQjSWpN+TEZFiBjXn4Pllg1U/lVGhDGnuTwYr3a3DavAiA5MFJ+yDBOOKvwIEt+swe2L1oPRtI44hzkbLqKBuVW3A== The signature field should be signed with the airline\u0026rsquo;s private key, but couldn’t find any information regarding this except this website.\nEDIT: Reading similar cases online like this, looks like additional information is sent by the airline to the airport system to let me go through.\n","permalink":"https://speeddragon.github.io/posts/bypass_tsa/","summary":"\u003cp\u003eOn a recent trip to the United States, I noticed there is a way to skip the TSA queue waiting for clearing immigration going out of the country, so I decided to share it here. But first I want to tell you how I ended up finding it.\u003c/p\u003e\n\u003cp\u003eI arrived at the airport and went to the TSA queue for my designated flight gate. Waited until it was my time to go through with my passport and online boarding pass. While my friend went without any issues, I didn’t have the same result. The TSA officer told me to go back to the airline check-in counter and ask them to reprint my boarding ticket. He also gave me this little paper.\u003c/p\u003e","title":"Jump the TSA queue at the airport"},{"content":"With the release of Elixir 1.10, the --partition option was added to the mix test to be able to split our test suit in runners.\nFor this use case, I’ve used ex_coveralls to generate the coverage report. To get the coverage for our test suite we need to export the .coverdata files for each runner, using --export-coverage option that is available since version 0.15.2\nWhen running with partitions you need to define MIX_TEST_PARTITION environment variable with a value between 1 and 4.\n# Repeat for 1 to 4 export MIX_TEST_PARTITION=1 mix coveralls --partitions 4 --export-coverage \u0026#34;run_$MIX_TEST_PARTITION\u0026#34; After each test run you will have 4 different .coverdata files inside the cover folder: run_1.coverdata, run_2.coverdata, run_3.coverdata, run_4.coverdata .\nAggregate cover data To generate a report with these files there are two options, a simple one and a more difficult one.\nSimple one We can aggregate and export these metrics by running any kind of test. Since ExCoveralls 0.15.3 we can import coverage data from other tests using --import_cover \u0026lt;folder\u0026gt; (normally the folder would be cover ).\nSince we don’t want to run any particular test again we can use the following command to not run any test but generate the coverage report.\nmix coveralls --no-start --exclude test --import-cover cover If you are just interested in the global coverage, you can extract it from coveralls.html using the following bash command:\nexport COVERAGE=$(cat cover/excoveralls.html | grep “percentage” | sed -n ‘2 p’ | grep -E -o ‘\\[0–9.\\]{1,5}’) Difficult one If for some reason you don’t want to set up everything to run mix coveralls there is another option, but keep in mind that you will need the .beam files, normally stored in _build/test/lib/\u0026lt;project_name\u0026gt;/ebin .\nThe following Elixir code enables you to combine all the .coverdata files in one file. It would be needed for the next command.\n# Import coverdata files File.ls!(\u0026#34;cover\u0026#34;) |\u0026gt; Enum.filter(\u0026amp;(String.contains?(\u0026amp;1, \u0026#34;.coverdata\u0026#34;))) |\u0026gt; Enum.each(fn file -\u0026gt; \u0026#34;cover/#{file}\u0026#34; |\u0026gt; String.to_charlist() |\u0026gt; :cover.import() end) # Debug :cover.imported() |\u0026gt; IO.inspect() # Export to one file :cover.export(\u0026#39;cover/all.coverdata\u0026#39;) Then we can generate the coverage report with Covertool. If you have issues compiling the code, you can useREBAR=\u0026quot;\u0026quot; make deps; make compile. Make sure you have OTP 25 installed.\n./covertool -cover cover/all.coverdata -ebin _build/test/lib/\u0026lt;project_name\u0026gt;/ebin This will generate a coverage.xml file with the coverage information. If you want to extract the global coverage you can use the following commands.\n# Extract line-rate value from file export COVERAGE_FLOAT=$(xq -r ‘.coverage.”@line-rate”’ coverage.xml) # Calculate percentage awk -vcoverage=$COVERAGE_FLOAT ‘BEGIN{print coverage*100}’ Conclusion I hope you find this useful to you. Let me know if you had issues making this work on your project.\n","permalink":"https://speeddragon.github.io/posts/coverage-with-partition-test-a7835a147b42/","summary":"\u003cp\u003eWith the \u003ca href=\"https://github.com/elixir-lang/elixir/releases/tag/v1.10.0\"\u003erelease\u003c/a\u003e of Elixir 1.10, the \u003ccode\u003e--partition\u003c/code\u003e option was added to the mix test to be able to split our test suit in runners.\u003c/p\u003e\n\u003cp\u003eFor this use case, I’ve used \u003ca href=\"https://github.com/parroty/excoveralls\"\u003eex_coveralls\u003c/a\u003e to generate the coverage report. To get the coverage for our test suite we need to export the \u003ccode\u003e.coverdata\u003c/code\u003e files for each runner, using \u003ccode\u003e--export-coverage\u003c/code\u003e option that is available since version 0.15.2\u003c/p\u003e\n\u003cp\u003eWhen \u003ca href=\"https://hexdocs.pm/mix/Mix.Tasks.Test.html#module-command-line-options\"\u003erunning with partitions\u003c/a\u003e you need to define \u003ccode\u003eMIX_TEST_PARTITION\u003c/code\u003e environment variable with a value between 1 and 4.\u003c/p\u003e","title":"Elixir code coverage with partition test"},{"content":"Nowadays, with the fast pacing appearance of services on the Internet, we are faced constantly with new challenges. More commonly we are seeing new types of attacks to companies that hold customer information. As today, this is an asset that keeps increasing in value.\nIn this article I will introduce you to my view and experience of gathering information since my early days of programming up until now, how it evolved, some challenges I’ve been faced with, and my view of the future.\nEarly Days Since my first steps on the Internet, I remember bots being present in the famous IRC clients and chatrooms. I was fascinated by how we could automate actions, pre-program some settings to help us complete some specific action and set up rules and parameters to help us reduce the time needed to perform what would be a manual action. These were my early days, the days I first saw a line of code and was something completely strange for me.\nWhat is a Bot ? An Internet bot, also known as web robot, WWW robot or simply bot, is a software application that runs automated tasks (scripts) over the Internet. Typically, bots perform tasks that are both simple and structurally repetitive, at a much higher rate than would be possible for a human alone.\nI remember questioning myself how the dialogs were created. The first time I “created” a dialog I think it was in MS Paint, using a print screen of a window (normally Internet Explorer, because it was the “portal” for accessing the Internet) and started adding buttons and changing text and input fields.\nI didn’t create many dialogs in mIRC, but I was fascinated how to automate a command, that could automatically reply to people messages. Creating a bot to reply to someone if a certain keyword was present was fun and then I started creating more complex actions in order to automate some repetitive tasks.\nWhen I stopped using mIRC, I remember that people created a lot of complex text games (maybe the predecessor of Omerta and other first internet web based games).\nWhat is a Crawler ? A crawler is a program that visits Web sites and reads their pages and other information in order to create entries for a search engine index. The major search engines on the Web all have such a program, which is also known as a “spider” or a “bot.”\nNormally crawlers are used in search engine to fetch information to be displayed on future searches. It used to be only GET requests that were indexed, but since 2011 that Google Bot also perform POST request to retrieve information, reaching further into the amount of information that can be gathered.\nThe growth of information access via Internet I start realising that the Internet was getting bigger and bigger, and a lot of systems were starting to become available to access them through a browser. Then I started to see bots being created to scrape text out of the server’s responses (well, that was how Google started) and people started building applications that gathered information from different systems, ultimately providing a service or an API out of it.\nAfter a while, companies realised that some actions needed to be taken, in order to check if the request was made by a human or a robot. In the early days, performing OCR on a CAPTCHA or understand what was inside of an image was pretty hard to do with a common computer, but soon that become an interest for the researchers and a problem easily overcome with the additional computational power available in the following years.\nGoogle ReCaptcha is probably the most widely used system today to prevent robots to access areas of websites that are meant to be accessed by humans only. There are other solutions, some of them are easy to bypass, and some that are very difficult for both humans and robots to solve it.\nStages of developing an information gathering service You can split the development of a bot/crawler into several stages. The first one starts with the planning, then you have the process of extracting the information and storage. Then you start to know how the website behaves and learn how to run it reliable 24/7.\nAdditional steps can be taken in order to help you visualise or learn more about the information that you are gathering.\nStage 1: Information recognition The first stage is to understand what information is available to you. Then you will learn how the website behaves, and pinpoint every detail that can be useful into the process of developing your bot.\nCheck every visual information that you can extract (text, images, videos, …). Check for hidden information that you can extract. Not all information is visible to the naked eye and sometimes you need to lurk in the source code to catch some extra information hidden in form fields or with CSS. Check your browser network tab for calls to an API, that may contain extra information that isn’t displayed in the website. In the olds days client side rendering wasn’t a major standard as it is today, so normally what you saw was what you got. Nowadays REST APIs are everywhere and a lot of information is returned in these calls that aren’t displayed visually to the end user. Check if you can differ an empty/invalid response from a not working service response. In some services the response of not having any information to show is the same as the service being down. The best way to recognise this is to log every request made and search for patterns in the data stored. This will also reveal possible service schedule maintenance patterns that might occur during the collection process. Check for possible visible or invisible automation detection system (like ReCaptcha, CloudFlare, etc). This will help assess how viable it will be to implement a crawler/bot on that page. At the end of this step, you should have all the necessary information to model your data into tables and columns in your database. Depending how much data you plan to extract, it might be good to have some experience in database optimisation, or to start reading about it.\nStage 2: Mimic browser behaviour This could be a painful process depending how many requests exists and how many static/dynamic fields you need to send back for verification / validation purposes. At this stage you should select your favourite programming language and start using an HTTP client to try to mimic the browser behaviour.\nTo help you in this hard task, you have two great tools at your disposal: the browser inspect (Chrome, Firefox and Safari) and Burp Suite Community Edition.\nYou should start by removing the headers parameters until you have the necessary minimum data to send in order to obtain the right response, but sometimes you can’t change anything. It might be picky and that can drive you insane until you have right information to be sent, especially if the service changes its behaviour.\nAccept-Language: pt-PT,pt;q=0.8,en;q=0.5,en-US;q=0.3\nAccept: text/html,application/xhtml+xml,application/xml; q=0.9,*/*; q=0.8\nBrowser Inspect When you start analysing the network requests, remember that you should open the network tab before the page is loaded, or if it was already loaded, make sure you refresh it so you can catch all network requests made inside that tab.\nInside the browser inspection tool, you will see which data is sent and received between your browser and the server, which websites it connects to, cookies, parameters sent, etc. Normally you want to find the request that have the information you want to extract (your goal), and them backtrack all the necessary steps from there, like for example search for early requests being made that must be replicated in your crawler.\nIn the majority of the cases you just need to know which request type is it (GET or POST, the most common ones) and which endpoint you are connecting to (URL), as well which parameters you need to input to extract the result returned by the server. Typically, this could be a form-data format like key=value\u0026amp;key2=value2 or a JSON format string input {\u0026quot;key\u0026quot;: \u0026quot;value\u0026quot;, \u0026quot;key2\u0026quot;: \u0026quot;value2\u0026quot;}.\nIf this doesn’t work, the next step is to investigate the cookies, and check if you need to do a previous request in order to generate the necessary cookies for the server to send the reply you are expecting. Sometimes this could be a long and painfully processes, so another way to do this is to export the request that return the information you want into a cURL command (on Chrome, right click in the request you want and select Copy \u0026gt; Copy as cURL), and check in your command line if that works. Then you start to remove what you think might be unnecessary data until the response isn’t the same. That’s how you find out what fields are required or not.\nBurp Suite Community Edition Burp is probably the best tool to help you understand what information is being sent from your browser into the server, using a proxy.\nThis is a powerful tool used by hackers, security researches, and other IT personal, in order to easily access all the requests being sent by your browser, as well as utilise the handful of features that allows them to tamper with requests and resend them, in order to better understand the server’s behaviour.\nStage 3: Information extraction automation After you have identified the data to extract and have it in your side (replicating the browser behaviour), the next step is to store it into your database.\nDon’t forget to sanitise all information retrieved from the website / API. You shouldn’t trust the information that you extracted, and you should handle it as “user input”. If you display it in a HTML page, convert all input into html entities. Don’t ever concatenate extracted data into a SQL query, use prepare statements. Try to follow security standards like a normal web application. Use a DOM parser with query selectors to extract the information if the response is in HTML format. If the response is in JSON format use the decoder available for your language. For example, in PHP you have json_decode($input) . There might be some cases, like Facebook where parsing HTML as an HTTP request might be difficult, because everything is rendered on the client side with complex/obfuscated JavaScript code. Since the Cambridge Analytic scandal, Facebook has been locking access to its GraphQL API data, making it harder and harder to get “clean” data from it.\nThe solution for this issues is to use Puppeteer, a API to the Google Chrome browser that allows you to control it using NodeJS. Its behaviour is similar as doing an HTTP request, but with much more functionality because of the JavaScript being executed in the client side. Absolutely worthwhile looking into it.\nThe following code is a NodeJS example of how to extract the biggest size image stored in Facebook using Puppeteer.\nAnd here is a great article how to use Puppeteer to bypass some Captcha sliders.\nStage 4: Error mapping and logging At this stage you will try to map all types of errors that the webpage or API can give into your bot, so it can recognise the difference between an expected and an unexpected error and behave accordantly.\nWhen an unexpected message arrives, my advise is to stop any further actions from being executed, then take some time to look and implement a new behaviour into the bot. This way you avoid having it doing unexpected things or store invalid/incomplete information without letting you know first. A simple email message should be enough to alert you.\nAnother consideration is to handle HTTP status, like 503 (Service Unavailable). Some APIs go to sleep during the night, or during certain hours or days for updates, deployments, maintenance, etc. Remember to keep some kind of log for each request (for example save every script output into a log file,/var/log/crawler1.log ) with timestamp to be able to analyse later the crawler behaviour.\nNormally I also recommend adding some DEBUG flags into your code, so you can enable it if you encounter a weird behaviour, and disable it for normal operation, only having minimal output with a timestamp, so you can know what is the time interval used between the requests and which data was fetched.\nStage 5: Starting it … You reached the point where you can provide some input, for example and identifier (id), and using a command line or other form of interaction you can retrieve information associated to that identifier, and then store it in the database. Now your plan is to automate all this process, mainly trying to disguise the bot’s behaviour as a human behaviour.\nConsiderations before running Probably the major aspect of keeping it running in a loop is the time interval between each request. How many requests are you making per day ? Per hour ? Per minute ? Per second ? You need to have in mind that the more requests you do, the more likely you will be detected. In the other hand, the less requests you do, less data you will have per unit of time, and more time you will need to reach your end goal (if your end goal is retrieve all the information stored on the server).\nHere are some scenarios you can face if you’re caught making to many requests. Their actions can be:\nAdd a limit to the number of requests made per account / ip address. Add captcha (like ReCaptcha) in order to access the information. Block your IP (you will probably easily bypass if you have a dynamic ip, or you can use a proxy or TOR). Remove the service from being online. The first couple of minutes are good to understand the behaviour of the bot. Keeping an eye on your bot logs is a good way to check if the intended behaviour is being applied successfully.\nCreate some statistics of its behaviour:\nTime since last valid request. Generate an alert if is higher than the normal amount. Number of request per hour, per day, per week. Difference between the number of requests of the last couple of days. Stage 6: Keep it running Your major concern is to avoid downtime in your bot due an unexpected behaviour. Here is a list of some actions you should take to better prepare for when an unexpected action is perform on their side.\nDown Notification One of the first things you need to set up is a notification when the website or API is down. This can be useful to understand possible downtime patterns.\nChange Notification After building a bot, you hope that you will do the least amount of maintenance as possible, but with the fast pace of the software development, every few months a new change might occur, like a modification in the HTML code, or the endpoints used. Still, the most difficult part is building it, and after that any change should be easy doable without losing much time, but you will still experience downtime.\nAvoid Detection / Block One way the servers can block you is by your IP address. Normally this is done via a firewall, IDS (Intrusion Detection System) or the service it self, if it has some kind of rate limit capabilities.\nOne of the most common ways to hide your IP is using a proxy service, and TOR offers you a great way to do that. You can specify which country you want to be your exit from the TOR network, if for example the service only accepts IP addresses from certain countries.\nI also recommend changing the TOR exit node IP address by restarting the TOR service several times a day. This is good for two reasons: some nodes are painfully slow and can trigger a timeout in your connection, and to increase the number of IP requested tracked on the server side. But remember that TOR exit nodes can be blocked, o be careful on the amount of request you do to crawl a service before they detect and disable this option.\nStorage You need to save the extracted data in some place. You could save in text files, but probably one of the reasons you are extracting information is to access it later, and maybe perform some analysis to it.\nYou have today a great offer of database engines, from relations databases like MySQL and PostgreSQL to different types of NoSQL databases like MongoDB, ElasticSearch, Redis, etc. You can investigate which one will be better for your use case, of choose the one you’re most familiar with and then see if further improvements need to be made down the line.\nMost of my experience with databases optimisation came from having slow queries when performing data analysis on extracted information. The database grew past the expected size, so new solutions needed to be found to keep the query times lower.\nThis can also happen if you plan to apply artificial intelligence to the data before making the next request. In some cases, depending which input are given to the service/page you can infer what values can be return based on which values you have.\nBellow are some possible actions to take into consideration to speed up data access:\nCache Process data on write, if you have an intensive read bot. Use EXPLAIN on your query if you are using MySQL or PostgreSQL. Check what is having a higher cost. Normally adding an index will speed things up, but also if your query is in a complex string, it might be good to split into two columns that using substring to filter it out. Stage 7: Dashboard / Visual Application You should be happy with everything running perfectly now, but in the end, you are just storing data on some tables in a database. An optional step could be the development of a dashboard to visualise the information gathered, adding some graphs, tables, search fields to query the data and create new queries to perform some analysis on the data you’ve retrieved.\nThis will help you better understand it and can also be integrated with other bots or services in the future to connect the data you already have with other sources.\nThis can be specially helpful if you are gathering media files like images, sounds or videos.\nFinal thoughts There is a lot of valuable information on the internet that should be better protected against crawlers and bots, and other information that should be publicly accessible via an API to be easily fetched and used to create useful applications.\nAs we go further into the future, we tend to share more and more of our lives in social media apps and websites, our location, our food, our home, our mood, our trips. There is nothing wrong if we really want to share this information, but we should be aware of how this data is handled by companies and who have access to this information, and what they can do with it.\nCompanies need to step up the responsibility to protect our data, since the beginning of their product. Now with GDPR in Europe, we are getting more ownership of our data, and what companies do with it. The responsibility of owning data needs to be in the company vision from now on, in order to avoid serious issues in the future, something similar of what we’ve experienced in Cambridge Analytica.\nPeople need to be more conscious on what the share online, setting permissions on who can see it and so on.\nOne thing I’m pretty sure, our lives will be less private in the future.\n","permalink":"https://speeddragon.github.io/posts/bots-and-crawlers-the-automation-of-information-gathering-e5afc88fb625/","summary":"\u003cp\u003eNowadays, with the fast pacing appearance of services on the Internet, we are faced constantly with new challenges. More commonly we are seeing new types of attacks to companies that hold customer information. As today, this is an asset that keeps increasing in value.\u003c/p\u003e\n\u003cp\u003eIn this article I will introduce you to my view and experience of gathering information since my early days of programming up until now, how it evolved, some challenges I’ve been faced with, and my view of the future.\u003c/p\u003e","title":"Bots and Crawlers — The automation of information gathering"},{"content":"\nHere it is, another small article about the implementation of a new functionality in the API service we are building using Elixir and Phoenix.\nImplementation We need to implement a way to upload user images to the cloud (S3 in this case) so after some research I found this article that explains in a very good way the basic code on how to implement it. Arc is a very good dependency to handle image upload either for local or for S3 storage.\nStore and fetch After installing all the dependencies mentioned on the article, you can have a look into a simple controller, in this case I only specify the index and create methods, but all other methods (for example update) are similar.\ndefmodule MyApp.Web.AvatarController do ... def **index**(conn) do # We use Guardian to fetch user information current_user = current_resource(conn) if current_user != nil do image_url = Avatar.url({\u0026#34;image.jpg\u0026#34;, current_user}) conn |\u0026gt; Phoenix.Controller.redirect(external: image_url) else send_resp(conn, :not_authorized, \u0026#34;\u0026#34;) end end def **create**(conn, %{\u0026#34;avatar\u0026#34; =\u0026gt; avatar}) do current_user = current_resource(conn) if avatar != nil do case Avatar.store({avatar, user}) do {:ok, _file_name} -\u0026gt; send_resp(conn, :ok, \u0026#34;\u0026#34;) _ -\u0026gt; send_resp(conn, :service_unavailable, \u0026#34;\u0026#34;) end else send_resp(conn, :bad_request, \u0026#34;\u0026#34;) end end end In this example, we start our methods by getting the user object. In this case we use Guardian, but you can get it from the database using Ecto. In our Avatar module generated by Arc using mix arc.g avatar we decided to incorporate the user ID into the file name.\ndefmodule MyApp.Avatar do ... # We use this so other users can\u0026#39;t check other user profile images def filename(version, {_file, scope}) do :crypto.hash(:sha256, \u0026#34;a_very_long_string_#{scope.id}_#{version}\u0026#34;) |\u0026gt; Base.encode16 |\u0026gt; String.downcase end # Override the storage directory: def storage_dir(_version, {_file, scope}) do \u0026#34;upload/user/avatars/#{scope.id}\u0026#34; end ... end Note that #{scope.id} will use the user.id field, be sure you have it or change it to the field that you want to use.\nAfter that, we check if the request has the avatar parameter, if not return a bad request HTTP status. If the parameter is present, we try to store it on S3 and we can check if it was successfully stored, checking the match pattern {:ok, filename} , in this case we could use {:ok, _} because we won’t use the filename.\nIntegration with Ecto To use Arc with Ecto, you also need to add arc_ecto dependency. To add the field in the user table schema you need to start by creating a new migration to insert a new field.\n# new_migration.exs defmodule MyApp.Repo.Migrations.NewMigration do use Ecto.Migration def change do alter table(:user) do add :avatar, :string end end end After that, you need to add a new field into your user model and use the cast_attachments to validate the image upload and store the necessary information into the database.\n# user.ex defmodule MyApp.User do use MyApp.Web, :model use Arc.Ecto.Schema schema \u0026#34;user\u0026#34; do ... field :image, MyApp.Avatar.Type end def changeset(user, params \\\\\\\\ %{}) do user |\u0026gt; Ecto.Changeset.cast(params, ...) |\u0026gt; cast_attachments(params, \\[:image\\]) end end Be aware that when insert a new user the user.id isn’t available until the object is inserted on the database. You can generate an UUID (for example time + random number) to be associated to the filename or you need to insert the values first and then execute an update with the image only.\nTo view the image URL on the JSON structure I’ve created a simple method to correctly display the URL.\ndefmodule MyApp.Web.UserView do ... def render(\u0026#34;show.json\u0026#34;, %{user: user}) do ‰{ \u0026#34;id\u0026#34;: user.id, \u0026#34;username\u0026#34;: user.username, \u0026#34;avatar\u0026#34;: render_image_url(user) } end def render_image_url(user) do if user.avatar != nil do Avatar.url({user.avatar.file_name, user}, :original) else nil end end end To test if it’s working you can try it using Postman to send a POST request with form-data selected on the body and selecting a image file to upload.\nTesting Writing tests In this example, I am going to write a simple upload test and check if it was successful or not.\ndefmodule MyApp.Web.AvatarControllerTest do ... test **\u0026#34;Uploading test\u0026#34;**, %{user: user} do upload = %Plug.Upload{path: \u0026#34;test/assets/user_avatar.jpg\u0026#34;, filename: \u0026#34;user_avatar.jpg\u0026#34;} post_params = %{\u0026#34;avatar\u0026#34; =\u0026gt; upload} conn = conn_build() |\u0026gt; post(avatar_path(conn_build(), :create), post_params) # In this case we send in the response the location URL of the image assert List.first(get_resp_header(conn, \u0026#34;location\u0026#34;)) == Avatar.url({user.id, user}) assert conn.status == 302 end In this test, we select an image from our test assets and upload it using a POST method. We get the response, in this case we check for 302 (redirect) because we return the final url into S3 storage.\nIn this second test I’m going to show a test using Ecto (and ExMachina to build the model).\ndefmodule MyApp.Web.UserTest do ... test **\u0026#34;Image Upload with Ecto\u0026#34;** do {:ok, avatar_struct} = MyApp.Avatar.Type.load(\u0026#34;x.jpg?1234567\u0026#34;) user = build(:user, image: avatar_struct) # Be sure the image is available in the test folder upload = %Plug.Upload{path: \u0026#34;test/assets/avatar_user_1.jpg\u0026#34;, filename: \u0026#34;avatar_user_1.jpg\u0026#34;} # POST parameters post_parameters = %{ \u0026#34;avatar\u0026#34; =\u0026gt; upload } conn = build_conn() |\u0026gt; post(user_path(build_conn(), :create), post_params) json_response(conn, :ok) == render_json(\u0026#34;show.json\u0026#34;, user: user) end defp **render_json**(template, assigns) do assigns = Map.new(assigns) MyApp.Web.UserView.render(template, assigns) |\u0026gt; Poison.encode! |\u0026gt; Poison.decode! end Using Fake S3 In order to implement tests to assure the behaviour of the functionality to upload images to AWS S3, we decided to use Fake S3, a fake AWS S3 API that replies in the same way as the real one, so we can test it at will without incurring in extra expenses and without an internet connection.\nThe readme file is pretty straight forward, and only two command lines are necessary to have it installed and running in your machine. After that you can check the code samples for different languages, in this case we are interested in Elixir. You can modify it on your config/test.exs and check if the port is the same as used in the fake server.\nAfter that ensure that the Fake S3 server is running with the following command fakes3 -r ~/.s3bucket -p 4567, before you perform the tests. Run mix test and check if everything is as expected.\nRunning in Travis To add this to your continuous delivery pipeline you can add the following lines into before_script, for example:\nbefore_script:- gem install fakes3- fakes3 -r $HOME/.s3bucket -p 4567 \u0026amp; To not let the service hanging we can kill it after the tests finished running.\nafter_script: — kill $(pgrep -f fakes3) Thank you for reading this, hope you liked it and learned something new!\nThank you so much for reading and if you enjoyed this article make sure to hit that 👏👏 button. It means a lot to us! Also don’t forget to follow Coletiv on Medium, Twitter, and LinkedIn as we keep posting more and more interesting articles on multiple technologies.\nIn case you don’t know, Coletiv is a software development studio from Porto specialised in Elixir, iOS, and Android app development. But we do all kinds of stuff. We take care of UX/UI design, web development, and even security for you.\nSo, let’s craft something together?\n","permalink":"https://speeddragon.github.io/posts/@speeddragon/phoenix-with-image-upload-to-s3-in-an-api-implementation-and-testing-6ab5187175b0/","summary":"\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/images/1__76JgbEfLsBsZ2FMXhh0IBQ.jpeg\"\u003e\u003c/p\u003e\n\u003cp\u003eHere it is, another small article about the implementation of a new functionality in the API service we are building using Elixir and Phoenix.\u003c/p\u003e\n\u003ch3 id=\"implementation\"\u003eImplementation\u003c/h3\u003e\n\u003cp\u003eWe need to implement a way to upload user images to the cloud (S3 in this case) so after some research I found \u003ca href=\"https://medium.com/@Stephanbv/elixir-phoenix-uploading-images-to-aws-s3-with-arc-72da8049e844\"\u003ethis article\u003c/a\u003e that explains in a very good way the basic code on how to implement it. \u003ca href=\"https://github.com/stavro/arc\"\u003eArc\u003c/a\u003e is a very good dependency to handle image upload either for local or for S3 storage.\u003c/p\u003e","title":"Phoenix with image upload to S3 in an API: Implementation and testing"}]